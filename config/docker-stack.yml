version: '3.5'
services:

  ## registry service for storing and serving built images locally to the swarm
  #  registry:
  #    image: registry:2
  # ports:
  #   - target: 443
  #     published: 443
  #     protocol: tcp
  #     mode: ingress
  # environment:
  #   REGISTRY_HTTP_TLS_CERTIFICATE: /certs/domain.crt
  #   REGISTRY_HTTP_TLS_KEY: /certs/domain.key
  #   #REGISTRY_HTTP_ADDR: 0.0.0.0:443
  # volumes:
  #   - resources/registry/certs:/certs
  #   - resources/registry/images:/var/lib/registry
  # deploy:
  #   replicas: 1
  #   placement:
  #     constraints: [node.role == manager]
  # networks:
  #   ute_net:


  ## Redis database and message broker for Celery. don't modify unless you know
  ## what you're doing
  redis:
    image: ute:redis
    ports:
      - target: 6379
        published: 6379
        protocol: tcp
        mode: ingress
    deploy:
      replicas: 1
      placement:
        constraints: [node.role == manager]
    networks:
      ute_net:


  ## docker stats web UI
  #  redis-stats:
  #  depends_on:
  #    - redis
  #  image: insready/redis-stat:latest
  #  ports:
  #    - target: 63790
  #      published: 8080
  #      protocol: tcp
  #      mode: ingress
  #  deploy:
  #    replicas: 1
  #    placement:
  #      constraints: [node.role == manager]
  #  networks:
  #    ute_net:



  ## tasker drives the worker tasking by building jobs from files and 
  ## distributing the jobs as tasks via RabbitMQ. also responsible for 
  ## building final video file
  tasker:
    depends_on:
      - redis
    image: ute:tasker
    volumes:
      - nfs-in:/ute/inbox
      - nfs-out:/ute/outbox
    networks:
      ute_net:
    deploy:
      replicas: 1
      placement:
        constraints: [node.role == manager]

  ## defines a task to run the celery worker container in. these tasks are 
  ## replicated through the docker swarm nodes the 'cpus' key reflects how 
  ## much cputime each worker is allowed to consume on the host. remove for 
  ## auto-allocation, or dial in for more control per worker (same for memory).
  ##
  ## a worker service can be defined for each host (and resources customized 
  ## accordingly), if you create another worker service named something else
  ## (like 'worker-server2'), and then under placement.constraints, add the
  ## name of the label you gave that system (accomplished by doing something
  ## like 'docker node update --label-add name=server2 <node_name>'). see 
  ## https://docs.docker.com/engine/reference/commandline/service_update/ for
  ## details.
  worker-smaug:
    environment:
      UTE_HOSTNAME: smaug
    depends_on:
      - redis
    image: ute:worker
    volumes:
      - nfs-in:/ute/inbox
      - nfs-out:/ute/outbox
      - x265-vol:/ute/x265
    networks:
      ute_net:
    deploy:
      mode: replicated
      replicas: 6
      placement:
        constraints:
          - node.labels.name == smaug
      resources:
        limits:
          cpus: '8.0'
          memory: 2000M


  ## worker service with resource values for a different system. service name
  ## must be different from other worker service.
  worker-nuc:
    environment:
      UTE_HOSTNAME: nuc
    depends_on:
      - redis
    image: ute:worker
    volumes:
      - nfs-in:/ute/inbox
      - nfs-out:/ute/outbox
      - x265-vol:/ute/x265
    networks:
      ute_net:
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.labels.name == nuc
      resources:
        limits:
          cpus: '4'
          memory: 2000M



  ## worker service with resource values for a different system. service name
  ## must be different from other worker service.
  worker-esx:
    environment:
      UTE_HOSTNAME: esx
    depends_on:
      - redis
    image: ute:worker
    volumes:
      - nfs-in:/ute/inbox
      - nfs-out:/ute/outbox
      - x265-vol:/ute/x265
    networks:
      ute_net:
    deploy:
      mode: replicated
      replicas: 2
      placement:
        constraints:
          - node.labels.name == esx
      resources:
        limits:
          cpus: '6'
          memory: 2000M


  ## global service (one service instance per swarm host) to build x265 in a
  ## local volume, to be accessed by any local container
  x265-builder:
    image: ute:x265-builder
    volumes:
      - x265-vol:/ute/x265
    networks:
      ute_net:
    deploy:
      mode: global
      restart_policy:
        condition: none


## create the 'ute_net' network if it doesn't exist yet. this is a docker 
## swarm "overlay" network that all services described here will be attached to
networks:
  ute_net:
    name: ute_net
    driver: overlay
    attachable: true


## point participating containers (namely the workers) at the NFS container
## we setup up above, as a "local volume".
##
## WARNING: if your NFS settings change for some reason, you must run "docker 
## volume rm <nfs volumes>" on each physical node in the swarm to remove the 
## old NFS mounts. otherwise new NFS settings will not take effect.
volumes:
  x265-vol:
    driver: local

  nfs-in:
    driver: local
    driver_opts:
      type: nfs
      o: addr=10.168.1.111,rw,nolock,noresvport
      device: ":/mnt/user/work/ute/inbound"
 
  nfs-out:
    driver: local
    driver_opts:
      type: nfs
      o: addr=10.168.1.111,rw,nolock,noresvport
      device: ":/mnt/user/work/ute/outbound"
