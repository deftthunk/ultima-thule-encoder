version: '3.5'
services:

  ## RabbitMQ server image is built and deployed
  ## relies on rabbitmq:alpine image
  rabbitmq:
    image: ute:rabbitmq
    ports:
      - target: 5672
        published: 5672
        protocol: tcp
        mode: ingress
      - target: 15672
        published: 15672
        protocol: tcp
        mode: ingress
    deploy:
      replicas: 1
      placement:
        constraints: [node.role == manager]
    networks:
      ute_net:


  ## defines the flower container for monitoring RabbitMQ
  ## flower will wait for rabbitmq to start first, and then attempt
  ## connection
  flower:
    depends_on:
      - "rabbitmq"
    image: mher/flower:latest
    command: "--broker=amqp://utbot:ultimaThule@rabbitmq:5672/utbot_vhost
              --broker-api=http://utbot:ultimaThule@rabbitmq:15672/api/"
    deploy:
      replicas: 1
      placement:
        constraints: [node.role == manager]
    ports:
      - target: 5555
        published: 5555
        protocol: tcp
        mode: host
    networks:
      ute_net:


  ## tasker drives the worker tasking by building jobs from
  ## files and distributing the jobs as tasks via RabbitMQ.
  ## also responsible for building final video file
  tasker:
    depends_on:
      - "rabbitmq"
    image: ute:tasker
    volumes:
      - "nfs-in:/inbound"
      - "nfs-out:/outbound"
    networks:
      ute_net:
    deploy:
      replicas: 1
      placement:
        constraints: [node.role == manager]
 

  ## defines a task to run the celery worker container in.
  ## these tasks are replicated through the docker swarm nodes
  ## the 'cpus' key reflects how much cputime each worker is allowed
  ## to consume on the host. remove for auto-allocation, or dial in
  ## for more control per worker (same goes for memory)
  worker:
    depends_on:
      - "rabbitmq"
    image: ute:worker
    volumes:
      - "nfs-in:/inbound"
      - "nfs-out:/outbound"
    networks:
      ute_net:
    deploy:
      mode: replicated
      replicas: 2
      resources:
        limits:
          cpus: '1.80'
          memory: 1000M
 

# create the 'ute_net' network if it doesn't exist yet
networks:
  ute_net:
    name: ute_net
    driver: overlay
    attachable: true


## point participating containers (namely the workers) at the NFS container
## we setup up above, as a "local volume"
volumes:
  nfs-in:
    driver: local
    driver_opts:
      type: nfs
      o: addr=192.168.1.209,rw,nolock,noresvport
      device: ":/inbound"
 
  nfs-out:
    driver: local
    driver_opts:
      type: nfs
      o: addr=192.168.1.209,rw,nolock,noresvport
      device: ":/outbound"
